{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution -yopengl (/home/ache/anaconda3/lib/python3.9/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -yopengl (/home/ache/anaconda3/lib/python3.9/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: vit-pytorch in /home/ache/anaconda3/lib/python3.9/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: torch>=1.6 in /home/ache/anaconda3/lib/python3.9/site-packages (from vit-pytorch) (1.11.0+cu113)\r\n",
      "Requirement already satisfied: einops>=0.4.1 in /home/ache/anaconda3/lib/python3.9/site-packages (from vit-pytorch) (0.4.1)\r\n",
      "Requirement already satisfied: torchvision in /home/ache/anaconda3/lib/python3.9/site-packages (from vit-pytorch) (0.12.0+cu113)\r\n",
      "Requirement already satisfied: typing-extensions in /home/ache/anaconda3/lib/python3.9/site-packages (from torch>=1.6->vit-pytorch) (4.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/ache/anaconda3/lib/python3.9/site-packages (from torchvision->vit-pytorch) (1.22.4)\r\n",
      "Requirement already satisfied: requests in /home/ache/anaconda3/lib/python3.9/site-packages (from torchvision->vit-pytorch) (2.26.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ache/anaconda3/lib/python3.9/site-packages (from torchvision->vit-pytorch) (9.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ache/anaconda3/lib/python3.9/site-packages (from requests->torchvision->vit-pytorch) (3.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ache/anaconda3/lib/python3.9/site-packages (from requests->torchvision->vit-pytorch) (2022.6.15)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ache/anaconda3/lib/python3.9/site-packages (from requests->torchvision->vit-pytorch) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ache/anaconda3/lib/python3.9/site-packages (from requests->torchvision->vit-pytorch) (1.26.7)\r\n",
      "\u001B[33mWARNING: Ignoring invalid distribution -yopengl (/home/ache/anaconda3/lib/python3.9/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -yopengl (/home/ache/anaconda3/lib/python3.9/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -yopengl (/home/ache/anaconda3/lib/python3.9/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -yopengl (/home/ache/anaconda3/lib/python3.9/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import psutil\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install vit-pytorch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  COLAB_ENV = True\n",
    "except:\n",
    "  COLAB_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "if COLAB_ENV:\n",
    "  start = \"/\" + os.path.join(\"content\",\"drive\",\"MyDrive\",\"datasets\")\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "else:\n",
    "  start = \"data\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_filepath = os.path.join(start, \"CERN\", \"ParticleClassification\")\n",
    "photon_file = \"SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
    "electron_file = \"SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
    "\n",
    "photon_filepath = os.path.join(data_filepath, photon_file)\n",
    "electron_filepath = os.path.join(data_filepath, electron_file)\n",
    "\n",
    "electronFile = h5py.File(electron_filepath, \"r\")\n",
    "photonFile = h5py.File(photon_filepath, \"r\")\n",
    "\n",
    "class PhotonElectronDataset(Dataset):\n",
    "    def __init__(self, _electron_filepath, _photon_filepath, transform=None, target_transform=None, low = 0, high = 1):\n",
    "        global seed\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        electronFile = h5py.File(_electron_filepath, \"r\")\n",
    "        photonFile = h5py.File(_photon_filepath, \"r\")\n",
    "\n",
    "        ElectronX, ElectronY = electronFile[\"X\"], electronFile[\"y\"]\n",
    "        PhotonX, PhotonY = photonFile[\"X\"], photonFile[\"y\"]\n",
    "\n",
    "        indexes = np.random.permutation(ElectronX.shape[0])\n",
    "\n",
    "        _low = int(low * len(indexes))\n",
    "        _high = int(high * len(indexes))\n",
    "\n",
    "        self.X = np.concatenate([ElectronX[_low:_high, :,:,:], PhotonX[_low:_high, :,:,:]])\n",
    "        self.Y = np.concatenate([ElectronY[_low:_high], PhotonY[_low:_high]])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #image = torch.from_numpy(self.X[idx, :, :, :])\n",
    "        image = self.X[idx, :, :, :]\n",
    "\n",
    "        label = self.Y[idx,]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 11.40256768\n",
      "Training dataset : 398400\n",
      "Validation dataset : 99600\n",
      "After 7.312928768\n"
     ]
    }
   ],
   "source": [
    "seed = 1231\n",
    "train_test_split = 0.8\n",
    "bs = 16\n",
    "\n",
    "print(f\"Before {psutil.virtual_memory()[1] / (1e9)}\")\n",
    "training_data = PhotonElectronDataset(electron_filepath, photon_filepath, transform=transforms.Compose([transforms.ToTensor()]), low=0, high=train_test_split)\n",
    "validation_data = PhotonElectronDataset(electron_filepath, photon_filepath, transform=transforms.Compose([transforms.ToTensor()]), low=train_test_split, high=1)\n",
    "\n",
    "print(f\"Training dataset : {len(training_data)}\")\n",
    "print(f\"Validation dataset : {len(validation_data)}\")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=bs, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=bs, shuffle=True)\n",
    "\n",
    "print(f\"After {psutil.virtual_memory()[1] / (1e9)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Used : cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device Used : {device}\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, saveTo, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    returnDict = {\"trainingLoss\" : [], \"validationLoss\" : [], \"trainingAccuracy\" : [], \"validationAccuracy\" : [],\n",
    "                  \"epochs\" : [], \"trainingAUC\" : [], \"validationAUC\" : []}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        returnDict[\"epochs\"].append(epoch)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            since2 = time.time()\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            y = torch.Tensor()\n",
    "            pred_y = torch.Tensor()\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.long()\n",
    "\n",
    "                y = torch.cat([y, labels.cpu()])\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    probs, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    pred_y = torch.cat([pred_y, outputs[:, 1].cpu()])\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            AUC_ROC = roc_auc_score(y.detach().numpy(), pred_y.detach().numpy())\n",
    "\n",
    "            if phase == \"train\":\n",
    "                returnDict[\"trainingLoss\"].append(epoch_loss)\n",
    "                returnDict[\"trainingAccuracy\"].append(epoch_acc.cpu().item())\n",
    "                returnDict[\"trainingAUC\"].append(AUC_ROC)\n",
    "            else:\n",
    "                returnDict[\"validationLoss\"].append(epoch_loss)\n",
    "                returnDict[\"validationAccuracy\"].append(epoch_acc.cpu().item())\n",
    "                returnDict[\"validationAUC\"].append(AUC_ROC)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} AUC ROC : {AUC_ROC:.4f} Time taken : {round(time.time() - since2, 2)}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model, saveTo)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    returnDict[\"best_model\"] = model\n",
    "    return returnDict\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def plotResults(results):\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.subplot(3,1, 1)\n",
    "    plt.plot(results['epochs'], results['trainingLoss'], label=\"training Loss\")\n",
    "    plt.plot(results['epochs'], results['validationLoss'], label=\"validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3,1, 2)\n",
    "    plt.plot(results['epochs'], results['trainingAccuracy'], label=\"training Accuracy\")\n",
    "    plt.plot(results['epochs'], results['validationAccuracy'], label=\"validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(3,1, 3)\n",
    "    plt.plot(results['epochs'], results['trainingAUC'], label=\"training ROC\")\n",
    "    plt.plot(results['epochs'], results['validationAUC'], label=\"validation AUC ROC\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ache/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]             640\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
      "           Dropout-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
      "              ReLU-6           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,928\n",
      "              ReLU-9             [-1, 64, 8, 8]               0\n",
      "        MaxPool2d-10             [-1, 64, 4, 4]               0\n",
      "          Flatten-11                 [-1, 1024]               0\n",
      "           Linear-12                   [-1, 64]          65,600\n",
      "    BasicCNNBlock-13                   [-1, 64]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]             640\n",
      "             ReLU-15           [-1, 64, 32, 32]               0\n",
      "        MaxPool2d-16           [-1, 64, 16, 16]               0\n",
      "          Dropout-17           [-1, 64, 16, 16]               0\n",
      "           Conv2d-18           [-1, 64, 16, 16]          36,928\n",
      "             ReLU-19           [-1, 64, 16, 16]               0\n",
      "        MaxPool2d-20             [-1, 64, 8, 8]               0\n",
      "           Conv2d-21             [-1, 64, 8, 8]          36,928\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "        MaxPool2d-23             [-1, 64, 4, 4]               0\n",
      "          Flatten-24                 [-1, 1024]               0\n",
      "           Linear-25                   [-1, 64]          65,600\n",
      "    BasicCNNBlock-26                   [-1, 64]               0\n",
      "           Linear-27                   [-1, 64]           8,256\n",
      "             ReLU-28                   [-1, 64]               0\n",
      "           Linear-29                    [-1, 2]             130\n",
      "          Softmax-30                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 288,578\n",
      "Trainable params: 288,578\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.22\n",
      "Params size (MB): 1.10\n",
      "Estimated Total Size (MB): 4.33\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.6220 Acc: 0.6635 AUC ROC : 0.7156 Time taken : 171.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ache/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5966 Acc: 0.6967 AUC ROC : 0.7508 Time taken : 16.17\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ache/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3655/3893642631.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0mexp_lr_scheduler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr_scheduler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStepLR\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer_ft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstep_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgamma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m results = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,{\"train\" : train_dataloader, \"val\": validation_dataloader},\n\u001B[0m\u001B[1;32m     62\u001B[0m                        {\"train\" : len(training_data), \"val\" : len(validation_data)}, saveTo=\"basicNN\", num_epochs=50)\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_3655/1689036542.py\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, saveTo, num_epochs)\u001B[0m\n\u001B[1;32m     42\u001B[0m                 \u001B[0;31m# track history if only in train\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_grad_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mphase\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'train'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m                     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m                     \u001B[0mprobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m                     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_3655/3893642631.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m         \u001B[0my1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcnn1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m         \u001B[0my2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcnn2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m         \u001B[0my3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0my1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_3655/3893642631.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m         \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    445\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    446\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 447\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    448\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    441\u001B[0m                             \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    442\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[0;32m--> 443\u001B[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0m\u001B[1;32m    444\u001B[0m                         self.padding, self.dilation, self.groups)\n\u001B[1;32m    445\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "class BasicCNNBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNNBlock, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class splittedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(splittedCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = BasicCNNBlock()\n",
    "        self.cnn2 = BasicCNNBlock()\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,2),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y1 = self.cnn1(x[:, 0, :, :].reshape(-1, 1, 32, 32))\n",
    "        y2 = self.cnn2(x[:, 1, :, :].reshape(-1, 1, 32, 32))\n",
    "        y3 = torch.cat([y1, y2], axis=1)\n",
    "        return self.pred(y3)\n",
    "\n",
    "\n",
    "model = splittedCNN()\n",
    "model = model.to(device)\n",
    "\n",
    "print(summary(model, input_size=(2, 32, 32)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "results = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,{\"train\" : train_dataloader, \"val\": validation_dataloader},\n",
    "                       {\"train\" : len(training_data), \"val\" : len(validation_data)}, saveTo=\"basicNN\", num_epochs=50)\n",
    "\n",
    "plotResults(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
